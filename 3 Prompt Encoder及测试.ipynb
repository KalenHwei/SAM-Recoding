{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f31ceb7f-94e0-4a1a-b409-5302c92bf8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from typing import Any, Optional, Tuple, Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "feb92a1d-2977-4912-ad86-1c4a98fcb44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 定义LayerNorm层\n",
    "\n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb5eb137-11d4-40fa-87a0-87d4a84d2d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 定义PositionEmbeddingRandom\n",
    "\n",
    "# 用高斯随机生成随机位置编码\n",
    "# 关于_position_embedding_encoding，它的输入都是实际坐标，因此在forward和forward函数调用之前要对coords进行处理，形成[d1, d2, ..., 2]的shape\n",
    "# 简单来说就是shape=[batch_size, N, 2]形状的坐标coords\n",
    "\n",
    "class PositionEmbeddingRandom(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_pos_feats: int = 64, # 位置编码的特征维度\n",
    "                 scale: Optional[float] = None\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        if scale is None or scale <= 0.0:\n",
    "            scale = 1.0\n",
    "        self.register_buffer( # 用到了register_buffer，即它不是一个参数，而是一个在前向传播过程中需要的固定值\n",
    "            \"positional_encoding_gaussian_matrix\", # 注册了一个名为这个的缓冲区张量\n",
    "            scale * torch.randn((2, num_pos_feats)), # 大小为(2, num_pos_feats=64)的高斯矩阵，并用scale缩放\n",
    "        )\n",
    "\n",
    "    # 对输入的坐标做随机位置编码，要求输入坐标已经标准化在[0,1]之间。该方法要求传入的是实际坐标coords，而不是坐标的size，即coords_size\n",
    "    def _position_embedding_encoding(self,\n",
    "                     coords: torch.Tensor # shape = [d1, d2, ..., 2]，前面的dx是网格，最后一个2存储的是点坐标，例如[0.1, 0.3]，前面的所有维度是用来储存点坐标的列表，例如[batch_size, N, 2] = [3, 3, 2]\n",
    "                     ) -> torch.Tensor:\n",
    "\n",
    "        coords = 2 * coords - 1 # 将coords的范围从[0,1]放缩到[-1,1]\n",
    "        coords = coords @ self.positional_encoding_gaussian_matrix # 例如[3, 3, 2] * [2, 64] = [3, 3, 64]\n",
    "        coords = 2 * np.pi * coords # shape = [3, 3, 64]不变\n",
    "        \n",
    "        return torch.cat([torch.sin(coords), torch.cos(coords)], dim=-1) # 放进sin和cos之后沿着最后一个维度拼成一个新的张量，shape = [3, 3, 128]\n",
    "\n",
    "    # 生成指定大小的网格位置编码，以_position_embedding_encoding()的例子为例，网格就应该是[3, 3]，输入参数是一个包含batch_size和N的元组Tuple(batch_size, N),以下简化为h和w的元组Tuple(h, w)\n",
    "    # 传入的是坐标size，即coords_size\n",
    "    def forward(self, \n",
    "                coords_size: Tuple[int, int] # 这个coords_size其实是(batch_size, N)，N为每个样本中的点数\n",
    "                ) -> torch.Tensor:\n",
    "\n",
    "        h, w = coords_size # 根据输入读取h和w\n",
    "        device: Any = self.positional_encoding_gaussian_matrix.device\n",
    "        \n",
    "        #创建一个shape=[h, w]的全1张量，并储存在device上\n",
    "        grid = torch.ones((h, w), device=device, dtype=torch.float32) # 假设生成一个[3, 3]的全为1的矩阵，命名为grid,并存储在device设备上，以便后续运算更新\n",
    "        x_embed = grid.cumsum(dim=1) - 0.5 # x轴坐标embedding为grid按行累加并减0.5，即每一行为[0.5, 1.5, 2.5]，shape = [3, 3]\n",
    "        y_embed = grid.cumsum(dim=0) - 0.5 # y轴坐标embedding为grid按列累加并减0.5，即每一列为[0.5, 1.5, 2.5]，shape = [3, 3]\n",
    "        x_embed = x_embed / w # 标准化，使其规范为[0, 1]之间，shape = (3, 3)\n",
    "        y_embed = y_embed / h # 标准化，使其规范为[0, 1]之间，shape = (3, 3)\n",
    "        \n",
    "        coords = torch.stack([x_embed, y_embed], dim=-1) # 在最后一列创建一个新的维度，stack在一起，得到coords，shape为[3, 3, 2]\n",
    "        position_embedding = self._position_embedding_encoding(coords) # 再将coords传入_position_embedding_encoding，得到position_embedding，shape = [3, 3, 128]\n",
    "        return position_embedding.permute(2, 0, 1)  # 调整为[128, 3, 3]，也就是 [C, H, W]\n",
    "\n",
    "    # 根据实际输入的坐标的shape和原始图像shape，标准化之后进行随机位置编码\n",
    "    # 传入的是坐标，即coords，和原始图像大小，即image_size\n",
    "    def forward_with_coords(self, \n",
    "                            coords_input: torch.Tensor, # shape = (batch_size, N, 2)\n",
    "                            image_size: Tuple[int, int]\n",
    "                           ) -> torch.Tensor:\n",
    "\n",
    "        coords = coords_input.clone()\n",
    "        coords[:, :, 0] = coords[:, :, 0] / image_size[1] # 标准化x轴的坐标\n",
    "        coords[:, :, 1] = coords[:, :, 1] / image_size[0] # 标准化y轴的坐标\n",
    "\n",
    "        position_embedding = self._position_embedding_encoding(coords.to(torch.float)) # B x N x C\n",
    "        \n",
    "        return position_embedding.permute(2, 0, 1) # 原版代码忘了permute了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ce47c44c-1073-42e9-acb0-fcec12339078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 定义Prompt Encoder\n",
    "\n",
    "class PromptEncoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim: int, # Prompt的Embedding Demension\n",
    "            image_embedding_size: Tuple[int, int], # Image的Embedding的尺度，例如(H, W)\n",
    "            input_image_size: Tuple[int, int], # 作为Image Encoder输入的pad之后的图像尺寸，例如(H, W)\n",
    "            mask_in_chans: int, # 隐藏channel数量，用于编码输入图像的mask\n",
    "            activation: Type[nn.Module] = nn.GELU, # 激活函数\n",
    "            ) -> None:\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.input_image_size = input_image_size\n",
    "        self.image_embedding_size = image_embedding_size\n",
    "        \n",
    "        self.positional_embedding_layer = PositionEmbeddingRandom(embed_dim // 2) # 也就是位置编码的特征维度num_pos_feats=64\n",
    "        \n",
    "        # 对输入的点和box编码\n",
    "        self.num_point_embeddings: int = 4  # 正例点、负例点、box的两个对角点\n",
    "        point_embeddings = [nn.Embedding(1, embed_dim) for i in range(self.num_point_embeddings)] # 对每个点做编码，词汇表大小为1，embeding大小为embde_dim\n",
    "        self.point_embeddings = nn.ModuleList(point_embeddings) # 编码器封装在self.point_embeddings里\n",
    "        self.not_a_point_embed = nn.Embedding(1, embed_dim) # ？\n",
    "        \n",
    "        # 对mask编码\n",
    "        self.mask_input_size = (4 * input_image_size[0], 4 * input_image_size[0]) # [4H, 4W]\n",
    "        self.mask_downscaling = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=mask_in_chans // 4, kernel_size=2, stride=2), # out_size = (input_size + 2*padding - kernel_size)/stride + 1，因此为[2H, 2W]\n",
    "            LayerNorm2d(mask_in_chans//4),\n",
    "            activation(),\n",
    "            nn.Conv2d(in_channels=mask_in_chans // 4, out_channels=mask_in_chans, kernel_size=2, stride=2), # 同上，变为[H, W]\n",
    "            LayerNorm2d(mask_in_chans),\n",
    "            activation(),\n",
    "            nn.Conv2d(in_channels=mask_in_chans, out_channels=embed_dim, kernel_size=1) # 同为[H, W]\n",
    "            )\n",
    "        self.no_mask_embed = nn.Embedding(1, embed_dim)\n",
    "        \n",
    "    def get_dense_positional_embedding(self) -> torch.Tensor:\n",
    "        '''\n",
    "        输出位置编码，用来对print prompt进行编码\n",
    "        这个位置编码应用于图像编码形状的稠密点集\n",
    "        \n",
    "        返回值：\n",
    "        Positional Encoding，shape = (1, embed_dim, embed_h, embed_w)\n",
    "        '''\n",
    "        return self.positional_embedding_layer(self.image_embedding_size).unsqueeze(0)\n",
    "    \n",
    "    # input(point prompt)做Embedding\n",
    "    def _embed_points(\n",
    "            self,\n",
    "            points: torch.Tensor, # shape = (batch_size, N, 2)\n",
    "            labels: torch.Tensor, # shape = (batch_size, N)\n",
    "            pad: bool, # 是否填充\n",
    "            ) -> torch.Tensor:\n",
    "        \n",
    "        points = points + 0.5  # 移动到像素的中心位置\n",
    "        if pad:\n",
    "            # 如果需要填充，创建填充值\n",
    "            padding_point = torch.zeros((points.shape[0], 1, 2), device=points.device)\n",
    "            padding_label = -torch.ones((labels.shape[0], 1), device=labels.device)\n",
    "            # 在点和标签的末尾添加填充值\n",
    "            points = torch.cat([points, padding_point], dim=1)\n",
    "            labels = torch.cat([labels, padding_label], dim=1)\n",
    "        # 使用坐标对点进行嵌入\n",
    "        point_embedding = self.positional_embedding_layer.forward_with_coords(points, self.input_image_size)\n",
    "        # 将标签为-1的点嵌入设置为0.0\n",
    "        point_embedding[labels == -1] = 0.0\n",
    "        # 对标签为-1的点添加`not_a_point_embed`的权重\n",
    "        point_embedding[labels == -1] += self.not_a_point_embed.weight\n",
    "        # 对标签为0的点添加`point_embeddings[0]`的权重\n",
    "        point_embedding[labels == 0] += self.point_embeddings[0].weight\n",
    "        # 对标签为1的点添加`point_embeddings[1]`的权重\n",
    "        point_embedding[labels == 1] += self.point_embeddings[1].weight\n",
    "        \n",
    "        return point_embedding\n",
    "\n",
    "    # 对input(box prompt)做embedding\n",
    "    def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        boxes = boxes + 0.5  # Shift to center of pixel\n",
    "        coords = boxes.reshape(-1, 2, 2)\n",
    "        corner_embedding = self.positional_embedding_layer.forward_with_coords(coords, self.input_image_size)\n",
    "        corner_embedding[:, 0, :] += self.point_embeddings[2].weight\n",
    "        corner_embedding[:, 1, :] += self.point_embeddings[3].weight\n",
    "        return corner_embedding\n",
    "\n",
    "    # 对input(mask prompt)做embedding\n",
    "    def _embed_masks(self, masks: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        mask_embedding = self.mask_downscaling(masks)\n",
    "        return mask_embedding\n",
    "\n",
    "    def _get_batch_size(\n",
    "        self,\n",
    "        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n",
    "        boxes: Optional[torch.Tensor],\n",
    "        masks: Optional[torch.Tensor],\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Gets the batch size of the output given the batch size of the input prompts.\n",
    "        \"\"\"\n",
    "        if points is not None:\n",
    "            return points[0].shape[0]\n",
    "        elif boxes is not None:\n",
    "            return boxes.shape[0]\n",
    "        elif masks is not None:\n",
    "            return masks.shape[0]\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def _get_device(self) -> torch.device:\n",
    "        return self.point_embeddings[0].weight.device\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n",
    "        boxes: Optional[torch.Tensor],\n",
    "        masks: Optional[torch.Tensor],\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Embeds different types of prompts, returning both sparse and dense\n",
    "        embeddings.\n",
    "\n",
    "        Arguments:\n",
    "          points (tuple(torch.Tensor, torch.Tensor) or none): point coordinates\n",
    "            and labels to embed.\n",
    "          boxes (torch.Tensor or none): boxes to embed\n",
    "          masks (torch.Tensor or none): masks to embed\n",
    "\n",
    "        Returns:\n",
    "          torch.Tensor: sparse embeddings for the points and boxes, with shape\n",
    "            BxNx(embed_dim), where N is determined by the number of input points\n",
    "            and boxes.\n",
    "          torch.Tensor: dense embeddings for the masks, in the shape\n",
    "            Bx(embed_dim)x(embed_H)x(embed_W)\n",
    "        \"\"\"\n",
    "        bs = self._get_batch_size(points, boxes, masks)\n",
    "        sparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device())\n",
    "        if points is not None:\n",
    "            coords, labels = points\n",
    "            point_embeddings = self._embed_points(coords, labels, pad=(boxes is None))\n",
    "            sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1)\n",
    "        if boxes is not None:\n",
    "            box_embeddings = self._embed_boxes(boxes)\n",
    "            sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)\n",
    "\n",
    "        if masks is not None:\n",
    "            dense_embeddings = self._embed_masks(masks)\n",
    "        else:\n",
    "            dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1).expand(\n",
    "                bs, -1, self.image_embedding_size[0], self.image_embedding_size[1]\n",
    "            )\n",
    "\n",
    "        return sparse_embeddings, dense_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
