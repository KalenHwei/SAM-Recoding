{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b2d67b33-2320-4642-863f-fad83f65d7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from typing import Optional, Tuple, Type\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "51fe4008-9aff-4458-a53b-1f1db95afb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.0 定义数据读取块ImageReader\n",
    "\n",
    "class ImageReader(nn.Module):\n",
    "    def __init__(self, \n",
    "                 size: Tuple = (1024, 1024)\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.reader = transforms.Compose([transforms.Resize(size), transforms.ToTensor()])\n",
    "        \n",
    "    def forward(self, x:str) -> torch.Tensor:\n",
    "        image = Image.open(x)\n",
    "        image_tensor = self.reader(image)\n",
    "        return image_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a58e8c38-652a-4987-9c0c-08d97fc8ed25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "# 0.0.1 测试ImageReader\n",
    "\n",
    "image = '/Users/kalen/Desktop/Python_env/segment-anything/cat2.jpg'\n",
    "reader = ReadImage()\n",
    "image_test = reader(image)\n",
    "print(image_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "78cfd01a-957c-444d-9eaa-747d5b4491f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1 前置函数1: 获取相对位置嵌入\n",
    "\n",
    "def get_rel_pos(q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    get_rel_pos函数用于根据q和k的size，获取相对位置嵌入\n",
    "    它的作用是捕捉输入序列中不同位置之间的相对关系\n",
    "    在注意力机制中，相对位置嵌入被用来增强模型对不同位置之间的依赖关系的建模能力\n",
    "    通过计算查询和键之间的相对坐标，然后根据相对坐标从相对位置嵌入中提取相应的位置嵌入\n",
    "    可以将这些位置嵌入添加到attention map中，从而影响注意力权重的计算\n",
    "    这有助于模型更好地理解输入序列中不同位置之间的关系，并提高模型在处理序列数据时的性能。\n",
    "    \n",
    "    参数解释:\n",
    "        q_size (int): size of query q.\n",
    "        k_size (int): size of key k.\n",
    "        rel_pos (Tensor): 相对位置嵌入 (L, C).\n",
    "\n",
    "    输出:\n",
    "        是根据查询和键的大小提取的相对位置嵌入\n",
    "    \"\"\"\n",
    "    max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
    "    # Interpolate rel pos if needed.\n",
    "    if rel_pos.shape[0] != max_rel_dist:\n",
    "        # Interpolate rel pos.\n",
    "        rel_pos_resized = F.interpolate(\n",
    "            rel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1),\n",
    "            size=max_rel_dist,\n",
    "            mode=\"linear\",\n",
    "        )\n",
    "        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)\n",
    "    else:\n",
    "        rel_pos_resized = rel_pos\n",
    "\n",
    "    # Scale the coords with short length if shapes for q and k are different.\n",
    "    q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n",
    "    k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n",
    "    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n",
    "\n",
    "    return rel_pos_resized[relative_coords.long()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4737564e-edc3-4d99-af48-a84944f602d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.2 前置函数2: 根据分解的相对位置嵌入调整attention map\n",
    "\n",
    "def add_decomposed_rel_pos(\n",
    "    scores: torch.Tensor,\n",
    "    q: torch.Tensor,\n",
    "    rel_pos_h: torch.Tensor,\n",
    "    rel_pos_w: torch.Tensor,\n",
    "    q_size: Tuple[int, int],\n",
    "    k_size: Tuple[int, int],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    计算分解后的相对位置嵌入\n",
    "        scores (Tensor): attention map，也就是torch.matmul(q, k_trans) / self.scale\n",
    "        q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).\n",
    "        rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n",
    "        rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n",
    "        q_size (Tuple): spatial sequence size of query q with (q_h, q_w).\n",
    "        k_size (Tuple): spatial sequence size of key k with (k_h, k_w).\n",
    "\n",
    "    Returns:\n",
    "        scores (Tensor): 加上了相对位置嵌入补偿的attention map\n",
    "    \"\"\"\n",
    "    q_h, q_w = q_size\n",
    "    k_h, k_w = k_size\n",
    "    Rh = get_rel_pos(q_h, k_h, rel_pos_h)\n",
    "    Rw = get_rel_pos(q_w, k_w, rel_pos_w)\n",
    "\n",
    "    B, _, dim = q.shape\n",
    "    r_q = q.reshape(B, q_h, q_w, dim)\n",
    "    rel_h = torch.einsum(\"bhwc,hkc->bhwk\", r_q, Rh)\n",
    "    rel_w = torch.einsum(\"bhwc,wkc->bhwk\", r_q, Rw)\n",
    "\n",
    "    scores = (\n",
    "        scores.view(B, q_h, q_w, k_h, k_w) + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n",
    "    ).view(B, q_h * q_w, k_h * k_w)\n",
    "\n",
    "    return scores           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "61ba8418-2e45-49ce-9482-944863d2af08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.3 前置函数3: 定义将图片切割成window的功能函数window_partition\n",
    "\n",
    "def window_partition(x: torch.Tensor, window_size: int) -> Tuple[torch.Tensor, Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    作用是将输入的张量（通常代表一个图像或特征图）划分成非重叠的小窗口，并在必要时对输入进行填充（padding），以确保窗口划分是完整的.\n",
    "    参数:\n",
    "        x (tensor): 输入，前端处理好的一般shape = (B=1, H, W, C) = (1, 16, 16, 768)\n",
    "        window_size (int): 每个窗口的大小\n",
    "\n",
    "    返回值:\n",
    "        windows: 划分后的窗口，形状为 [B * num_windows, window_size, window_size, C]\n",
    "        (Hp, Wp): 填充后的高度和宽度，用于后续处理或恢复原始尺寸\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape # 首先读取x的shape= (B=1, H, W, C) = (1, 16, 16, 768)\n",
    "\n",
    "    #得到需要填充的h和w的宽度\n",
    "    pad_h = (window_size - H % window_size) % window_size\n",
    "    pad_w = (window_size - W % window_size) % window_size\n",
    "    \n",
    "    #如果需要pad的话执行if语句\n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))\n",
    "    Hp, Wp = H + pad_h, W + pad_w\n",
    "\n",
    "    x = x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows, (Hp, Wp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6a4892e5-8305-4138-8220-a885312fded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.4 前置函数4: 定义将切割的window还原成图片的功能函数window_unpartition\n",
    "\n",
    "def window_unpartition(\n",
    "    windows: torch.Tensor, window_size: int, pad_hw: Tuple[int, int], hw: Tuple[int, int]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    将分割的window还原成原始序列，并移除padding\n",
    "    参数:\n",
    "        windows (tensor): 输入window，shape = (B*num_windows, window_size, window_size, C)\n",
    "        window_size (int): window size\n",
    "        pad_hw (Tuple): 填充的高和宽，用tuple封装，Tuple = (Hp, Wp)\n",
    "        hw (Tuple): 在padding之前的图片原始高宽(H, W)\n",
    "\n",
    "    输出值:\n",
    "        x: 还原切割之前的初始序列，shape = (B, H, W, C)\n",
    "    \"\"\"\n",
    "    Hp, Wp = pad_hw\n",
    "    H, W = hw\n",
    "    B = windows.shape[0] // (Hp * Wp // window_size // window_size)\n",
    "    x = windows.view(B, Hp // window_size, Wp // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, -1)\n",
    "\n",
    "    if Hp > H or Wp > W:\n",
    "        x = x[:, :H, :W, :].contiguous()\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8c2e20b2-9efb-4e1e-950c-a75f83d6a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 定义MLP块\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        mlp_dim: int,\n",
    "        act: Type[nn.Module] = nn.GELU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(embedding_dim, mlp_dim)\n",
    "        self.lin2 = nn.Linear(mlp_dim, embedding_dim)\n",
    "        self.act = act()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.lin2(self.act(self.lin1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "134e098f-81f3-4e02-bd4a-be2329207fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 定义LayerNorm块\n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1072dff7-b9a7-4f56-861b-0e01de030f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 定义Patch Embedding类，用卷积做\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            kernel_size: Tuple[int, int] = (16, 16),\n",
    "            stride: Tuple[int, int] = (16, 16),\n",
    "            padding: Tuple[int, int] = (0, 0),\n",
    "            in_chans: int = 3,\n",
    "            embed_dim: int = 768,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.projection = nn.Conv2d(in_channels=in_chans, out_channels=embed_dim, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # x:torch.Tensor表示输入x是Tensor，括号外面的-> torch.Tensor指函数返回值也是tensor\n",
    "        x = self.projection(x)\n",
    "        x = x.permute(0, 2, 3, 1) # 交换维度，即：(B C H W) -> (B H W C)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8c0013fa-794b-446e-a384-46d97d49ecd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64, 768])\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Patch Embedding的读入图片测试\n",
    "# 调用默认值\n",
    "patch_embed = PatchEmbed()\n",
    "#或自定义各个参数，即patch_embed = PatchEmbed(in_chans=3, embed_dim=768, kernel_size=(16, 16), stride=(16, 16), padding=(0, 0))\n",
    "\n",
    "output_from_Patch_Embed = patch_embed(image_test)\n",
    "print(output_from_Patch_Embed.shape) # 输出为shape = (B, H, W, dmodel) = (1, 64, 64, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2718e6-ab0d-49e3-99a8-76ec59f75ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "注意，根据Transformer论文定义，因此我把Embedding层从Encoder里拿出来了\n",
    "所以接下来的测试全部基于PatchEmbed的输出，也就是output_from_Patch_Embed来做\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f30cf2d2-1462-4698-bde2-079b98de1110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 更新的Attention类，用nn.Linear集成了可学习母参数以提高效率\n",
    "# 图片进入流程: image-tensorlizer-PatchEmbed-Attention\n",
    "# 因此最后的图片shape = (B=1, H. W. C) = (1, 64, 64, 768)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dmodel: int, # 也就是上面传进来的768\n",
    "                 num_heads: int = 8,\n",
    "                 qkv_bias: bool = True,\n",
    "                 use_rel_pos: bool = False,\n",
    "                 rel_pos_zero_init: bool = True,\n",
    "                 input_size: Optional[Tuple[int, int]] = None) -> None:\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        dmodel_per_head = dmodel // num_heads\n",
    "        self.scale = math.sqrt(dmodel_per_head)\n",
    "        \n",
    "        self.qkv = nn.Linear(dmodel, dmodel * 3, bias = qkv_bias) # 定义一个线性层\n",
    "        self.output_linear = nn.Linear(dmodel, dmodel)\n",
    "        \n",
    "        self.use_rel_pos = use_rel_pos\n",
    "        if self.use_rel_pos:\n",
    "            assert (\n",
    "                input_size is not None\n",
    "            ), \"如果用了相对位置编码，则必须提供输入的size\"\n",
    "            # initialize relative positional embeddings\n",
    "            self.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, dmodel_per_head), requires_grad = True)\n",
    "            self.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, dmodel_per_head), requires_grad = True)\n",
    "            \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, H, W, _ = x.shape # X的shape = (B, H, W, dmodel) = (1, 16, 16, 768)\n",
    "        qkv_combine = self.qkv(x) # shape = (B, H, W, 3*dmodel)，即用一个线性层生成qkv_combine\n",
    "        qkv_combine = qkv_combine.reshape(B, H*W, 3, self.num_heads, -1) # shape = (B, H*W, 3, num_heads, dmodel_per_head)\n",
    "        qkv_combine = qkv_combine.permute(2, 0, 3, 1, 4) # shape = (3, B, num_heads, H*W, dmodel_per_head)\n",
    "        qkv_combine = qkv_combine.reshape(3, B*self.num_heads, H*W, -1) # shape = (3, B*num_heads, H*W, dmodel_per_head)\n",
    "        q, k, v = qkv_combine.unbind(0) # q, k, v shape = (B*num_heads, H*W, dmodel_per_head)\n",
    "        \n",
    "        k_trans = k.transpose(-2, -1) # k_trans shape = (B*num_heads, dmodel_per_head, H*W)\n",
    "        scores = torch.matmul(q, k_trans) / self.scale # scores shape = (B*num_heads, H*W, H*W)\n",
    "        \n",
    "        # 如果使用了相对位置编码则启用\n",
    "        if self.use_rel_pos:\n",
    "            scores = add_decomposed_rel_pos(attn=scores, q=q, rel_pos_h=self.rel_pos_h, rel_pos_w=self.rel_pos_w, q_size=(H, W), k_size=(H, W))\n",
    "        \n",
    "        weights = torch.softmax(scores, dim = -1) # 在(B*num_heads, H*W, H*W)的最后一个维度上计算softmax。至于为什么已经推过了\n",
    "        output = torch.matmul(weights, v) # shape = (B*num_heads, H*W, dmodel_per_head)\n",
    "        output = output.view(B, self.num_heads, H, W, -1) # shape = (B, num_heads, H, W, dmodel_per_head)\n",
    "        output = output.permute(0, 2, 3, 1, 4) # (B, H, W, num_heads, dmodel_per_head)\n",
    "        output = output.reshape(B, H, W, -1) # shape = (B, H, W, dmodel)\n",
    "        output = self.output_linear(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d8a85cb9-4859-4b2e-8f84-e33b44be44c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64, 768])\n"
     ]
    }
   ],
   "source": [
    "# 4.1 测试Attention类\n",
    "\n",
    "# 定义参数\n",
    "dmodel = output_from_Patch_Embed.shape[-1]\n",
    "num_heads = 8\n",
    "qkv_bias = True\n",
    "use_rel_pos = False\n",
    "rel_pos_zero_init = False\n",
    "\n",
    "attn = Attention(dmodel=dmodel, num_heads=num_heads, qkv_bias=qkv_bias, use_rel_pos=use_rel_pos, rel_pos_zero_init=rel_pos_zero_init)\n",
    "\n",
    "output_from_Attention = attn(output_from_Patch_Embed)\n",
    "print(output_from_Attention.shape) # 成功输出，shape = (1, 64, 64, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "52d1ce14-4f04-4f47-9e15-c521373254f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 定义Vision Transformer Encoder Block\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dmodel: int,\n",
    "                 num_heads: int = 8,\n",
    "                 mlp_ratio: float = 4.0,\n",
    "                 qkv_bias: bool = True,\n",
    "                 norm_layer: Type[nn.Module] = nn.LayerNorm,\n",
    "                 act_layer: Type[nn.Module] = nn.GELU,\n",
    "                 use_rel_pos: bool = False,\n",
    "                 rel_pos_zero_init: bool = True,\n",
    "                 window_size: int = 0,\n",
    "                 input_size: Optional[Tuple[int, int]] = None,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dmodel)\n",
    "        self.Attention = Attention(dmodel,\n",
    "                                   num_heads = num_heads,\n",
    "                                   qkv_bias = qkv_bias,\n",
    "                                   use_rel_pos= use_rel_pos,\n",
    "                                   rel_pos_zero_init = rel_pos_zero_init,\n",
    "                                   input_size = input_size if window_size == 0 else (window_size, window_size),\n",
    "                                   )\n",
    "        self.norm2 = norm_layer(dmodel)\n",
    "        self.mlp = MLPBlock(embedding_dim = dmodel,\n",
    "                            mlp_dim = int(dmodel * mlp_ratio),\n",
    "                            act = act_layer\n",
    "                            )\n",
    "        self.window_size = window_size\n",
    "    \n",
    "    # 没有window：x - layernorm - Attention - short+x - x + mlp(layernorm(x))\n",
    "    # 有window：中间加两个变形和还原即可\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # x.shape = (B=1, H, W, dmodel) = (1, 16, 16, 768)\n",
    "        shortcut = x\n",
    "        \n",
    "        x = self.norm1(x)\n",
    "        if self.window_size > 0:\n",
    "            H, W = x.shape[1], x.shape[2]\n",
    "            x, pad_hw = window_partition(x, self.window_size) # 返回值为windows = (B * num_windows, window_size, window_size, C)和(Hp, Wp)\n",
    "        x = self.Attention(x)\n",
    "        if self.window_size > 0:\n",
    "            x = window_unpartition(x, self.window_size, pad_hw, ((H, W)))\n",
    "        \n",
    "        x = shortcut + x\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b4dcc093-85f7-4d54-80d9-f02b16055b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 64, 768])\n"
     ]
    }
   ],
   "source": [
    "# 5.1 测试Block\n",
    "\n",
    "# 定义参数\n",
    "dmodel = output_from_Patch_Embed.shape[-1]\n",
    "num_heads = 8\n",
    "mlp_ratio = 4.0\n",
    "qkv_bias = True\n",
    "norm_layer = nn.LayerNorm\n",
    "act_layer = nn.GELU\n",
    "use_rel_pos: bool = False\n",
    "rel_pos_zero_init = True\n",
    "window_size = 0\n",
    "input_size = None\n",
    "\n",
    "block = Block(dmodel=dmodel,\n",
    "              num_heads=num_heads,\n",
    "              mlp_ratio=mlp_ratio,\n",
    "              qkv_bias=qkv_bias,\n",
    "              norm_layer=norm_layer,\n",
    "              act_layer=act_layer,\n",
    "              use_rel_pos=use_rel_pos,\n",
    "              rel_pos_zero_init=rel_pos_zero_init,\n",
    "              window_size=window_size,\n",
    "              input_size=input_size)\n",
    "\n",
    "output_from_Block = block(output_from_Patch_Embed)\n",
    "print(output_from_Block.shape) # 成功输出，shape = (1, 64, 64, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "db120d1d-5ead-4ec0-978f-25a24f52b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 Vision Transformer Encoder 完整整合\n",
    "\n",
    "class ImageEncoderViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int = 1024,\n",
    "        patch_size: int = 16,\n",
    "        in_chans: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "        depth: int = 12,\n",
    "        num_heads: int = 12,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        out_chans: int = 256,\n",
    "        qkv_bias: bool = True,\n",
    "        norm_layer: Type[nn.Module] = nn.LayerNorm,\n",
    "        act_layer: Type[nn.Module] = nn.GELU,\n",
    "        use_abs_pos: bool = True,\n",
    "        use_rel_pos: bool = False,\n",
    "        rel_pos_zero_init: bool = True,\n",
    "        window_size: int = 0,\n",
    "        global_attn_indexes: Tuple[int, ...] = (),\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.patch_embed = PatchEmbed(\n",
    "            kernel_size=(patch_size, patch_size),\n",
    "            stride=(patch_size, patch_size),\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "        )\n",
    "\n",
    "        self.pos_embed: Optional[nn.Parameter] = None\n",
    "        if use_abs_pos:\n",
    "            # Initialize absolute positional embedding with pretrain image size.\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                torch.zeros(1, img_size // patch_size, img_size // patch_size, embed_dim)\n",
    "            )\n",
    "\n",
    "        self.blocks = nn.ModuleList() # 先生成一个空的Module，名称为blocks\n",
    "        for i in range(depth):\n",
    "            block = Block(\n",
    "                dmodel=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                norm_layer=norm_layer,\n",
    "                act_layer=act_layer,\n",
    "                use_rel_pos=use_rel_pos,\n",
    "                rel_pos_zero_init=rel_pos_zero_init,\n",
    "                window_size=window_size if i not in global_attn_indexes else 0,\n",
    "                input_size=(img_size // patch_size, img_size // patch_size),\n",
    "            )\n",
    "            self.blocks.append(block) # 再append进blocks里\n",
    "        \n",
    "        #出Block之后，经历卷积-标准化-卷积-标准化\n",
    "        self.neck = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                embed_dim,\n",
    "                out_chans,\n",
    "                kernel_size=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            LayerNorm2d(out_chans),\n",
    "            nn.Conv2d(\n",
    "                out_chans,\n",
    "                out_chans,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            LayerNorm2d(out_chans),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.pos_embed is not None:\n",
    "            x = x + self.pos_embed\n",
    "\n",
    "        for blk in self.blocks: # 执行Block\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.neck(x.permute(0, 3, 1, 2))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a636c15c-494b-4c01-bd37-836ed2f558fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 256, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# 6.1 测试ViT\n",
    "\n",
    "# 太多了所以写个测试函数\n",
    "def test_image_encoder_vit(x):\n",
    "    # 创建一个随机的输入张量，形状为 (1, 64, 64, 768)\n",
    "    input_tensor = x\n",
    "    \n",
    "    # 实例化 ImageEncoderViT\n",
    "    model = ImageEncoderViT(\n",
    "        img_size=1024,       # 图像尺寸\n",
    "        patch_size=16,       # patch 尺寸\n",
    "        in_chans=3,          # 输入通道数\n",
    "        embed_dim=768,       # 嵌入维度\n",
    "        depth=12,            # Transformer block 的深度\n",
    "        num_heads=12,        # 注意力头的数量\n",
    "        mlp_ratio=4.0,       # MLP 隐藏层的维度和嵌入维度的比率\n",
    "        out_chans=256,       # 输出通道数\n",
    "        qkv_bias=True,       # Query, Key, Value 的 bias\n",
    "        norm_layer=nn.LayerNorm, # 归一化层\n",
    "        act_layer=nn.GELU,   # 激活函数\n",
    "        use_abs_pos=True,    # 是否使用绝对位置嵌入\n",
    "        use_rel_pos=False,   # 是否使用相对位置嵌入\n",
    "        rel_pos_zero_init=True, # 相对位置参数是否初始化为零\n",
    "        window_size=0,       # 窗口大小\n",
    "        global_attn_indexes=()  # 使用全局注意力的块索引\n",
    "    )\n",
    "    \n",
    "    # 将输入张量传递给模型\n",
    "    output_tensor = model(input_tensor)\n",
    "    \n",
    "    # 输出结果的形状\n",
    "    print(f\"Output shape: {output_tensor.shape}\")\n",
    "\n",
    "# 运行测试\n",
    "test_image_encoder_vit(output_from_Patch_Embed) # 测试成功，输出为 torch.Size([1, 256, 64, 64])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
