{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b2d67b33-2320-4642-863f-fad83f65d7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from typing import Optional, Tuple, Type\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "78cfd01a-957c-444d-9eaa-747d5b4491f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1 前置函数1: 获取相对位置嵌入\n",
    "\n",
    "def get_rel_pos(q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    get_rel_pos函数用于根据q和k的size，获取相对位置嵌入\n",
    "    它的作用是捕捉输入序列中不同位置之间的相对关系\n",
    "    在注意力机制中，相对位置嵌入被用来增强模型对不同位置之间的依赖关系的建模能力\n",
    "    通过计算查询和键之间的相对坐标，然后根据相对坐标从相对位置嵌入中提取相应的位置嵌入\n",
    "    可以将这些位置嵌入添加到attention map中，从而影响注意力权重的计算\n",
    "    这有助于模型更好地理解输入序列中不同位置之间的关系，并提高模型在处理序列数据时的性能。\n",
    "    \n",
    "    参数解释:\n",
    "        q_size (int): size of query q.\n",
    "        k_size (int): size of key k.\n",
    "        rel_pos (Tensor): 相对位置嵌入 (L, C).\n",
    "\n",
    "    输出:\n",
    "        是根据查询和键的大小提取的相对位置嵌入\n",
    "    \"\"\"\n",
    "    max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
    "    # Interpolate rel pos if needed.\n",
    "    if rel_pos.shape[0] != max_rel_dist:\n",
    "        # Interpolate rel pos.\n",
    "        rel_pos_resized = F.interpolate(\n",
    "            rel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1),\n",
    "            size=max_rel_dist,\n",
    "            mode=\"linear\",\n",
    "        )\n",
    "        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)\n",
    "    else:\n",
    "        rel_pos_resized = rel_pos\n",
    "\n",
    "    # Scale the coords with short length if shapes for q and k are different.\n",
    "    q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n",
    "    k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n",
    "    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n",
    "\n",
    "    return rel_pos_resized[relative_coords.long()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4737564e-edc3-4d99-af48-a84944f602d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.2 前置函数2: 根据分解的相对位置嵌入调整attention map\n",
    "\n",
    "def add_decomposed_rel_pos(\n",
    "    scores: torch.Tensor,\n",
    "    q: torch.Tensor,\n",
    "    rel_pos_h: torch.Tensor,\n",
    "    rel_pos_w: torch.Tensor,\n",
    "    q_size: Tuple[int, int],\n",
    "    k_size: Tuple[int, int],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    计算分解后的相对位置嵌入\n",
    "        scores (Tensor): attention map，也就是torch.matmul(q, k_trans) / self.scale\n",
    "        q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).\n",
    "        rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n",
    "        rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n",
    "        q_size (Tuple): spatial sequence size of query q with (q_h, q_w).\n",
    "        k_size (Tuple): spatial sequence size of key k with (k_h, k_w).\n",
    "\n",
    "    Returns:\n",
    "        scores (Tensor): 加上了相对位置嵌入补偿的attention map\n",
    "    \"\"\"\n",
    "    q_h, q_w = q_size\n",
    "    k_h, k_w = k_size\n",
    "    Rh = get_rel_pos(q_h, k_h, rel_pos_h)\n",
    "    Rw = get_rel_pos(q_w, k_w, rel_pos_w)\n",
    "\n",
    "    B, _, dim = q.shape\n",
    "    r_q = q.reshape(B, q_h, q_w, dim)\n",
    "    rel_h = torch.einsum(\"bhwc,hkc->bhwk\", r_q, Rh)\n",
    "    rel_w = torch.einsum(\"bhwc,wkc->bhwk\", r_q, Rw)\n",
    "\n",
    "    scores = (\n",
    "        scores.view(B, q_h, q_w, k_h, k_w) + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n",
    "    ).view(B, q_h * q_w, k_h * k_w)\n",
    "\n",
    "    return scores           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8c2e20b2-9efb-4e1e-950c-a75f83d6a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 定义MLP块\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        mlp_dim: int,\n",
    "        act: Type[nn.Module] = nn.GELU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(embedding_dim, mlp_dim)\n",
    "        self.lin2 = nn.Linear(mlp_dim, embedding_dim)\n",
    "        self.act = act()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.lin2(self.act(self.lin1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "134e098f-81f3-4e02-bd4a-be2329207fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 定义LayerNorm块\n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1072dff7-b9a7-4f56-861b-0e01de030f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 定义Patch Embedding类，用卷积做\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            kernel_size: Tuple[int, int] = (16, 16),\n",
    "            stride: Tuple[int, int] = (16, 16),\n",
    "            padding: Tuple[int, int] = (0, 0),\n",
    "            in_chans: int = 3,\n",
    "            embed_dim: int = 768,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.projection = nn.Conv2d(in_channels=in_chans, out_channels=embed_dim, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # x:torch.Tensor表示输入x是Tensor，括号外面的-> torch.Tensor指函数返回值也是tensor\n",
    "        x = self.projection(x)\n",
    "        x = x.permute(0, 2, 3, 1) # 交换维度，即：(B C H W) -> (B H W C)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d9248cca-2ff4-4d47-80b5-10d7b0055659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Patch Embedding的读入图片测试\n",
    "image = Image.open('/Users/kalen/Desktop/Python_env/segment-anything/cat2.jpg')\n",
    "\n",
    "# 创建PatchEmbed实例\n",
    "patch_embed = PatchEmbed() # 调用默认值\n",
    "#或自定义各个参数，即patch_embed = PatchEmbed(in_chans=3, embed_dim=768, kernel_size=(16, 16), stride=(16, 16), padding=(0, 0))\n",
    "transform = transforms.Compose([transforms.Resize((256, 256)),\n",
    "                                transforms.ToTensor()\n",
    "                                ])\n",
    "\n",
    "image_tensor = transform(image)\n",
    "image_tensor = image_tensor.unsqueeze(0)\n",
    "output = patch_embed(image_tensor)\n",
    "print(output.shape) #  shape = (B H W C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f30cf2d2-1462-4698-bde2-079b98de1110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 更新的Attention类，用nn.Linear集成了可学习母参数以提高效率\n",
    "\n",
    "# 图片进入Attention之前的流程\n",
    "# image-tensorlizer-PatchEmbed-Attention\n",
    "# 最后的图片shape=(B=1, H. W. C) = (1, 16, 16, 768)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dmodel: int, # 也就是上面传进来的768\n",
    "                 num_heads: int = 8,\n",
    "                 qkv_bias: bool = True,\n",
    "                 use_rel_pos: bool = False,\n",
    "                 rel_pos_zero_init: bool = True,\n",
    "                 input_size: Optional[Tuple[int, int]] = None) -> None:\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        dmodel_per_head = dmodel // num_heads\n",
    "        self.scale = math.sqrt(dmodel_per_head)\n",
    "        \n",
    "        self.qkv = nn.Linear(dmodel, dmodel * 3, bias = qkv_bias) # 定义一个线性层\n",
    "        self.output_linear = nn.Linear(dmodel, dmodel)\n",
    "        \n",
    "        self.use_rel_pos = use_rel_pos\n",
    "        if self.use_rel_pos:\n",
    "            assert (\n",
    "                input_size is not None\n",
    "            ), \"如果用了相对位置编码，则必须提供输入的size\"\n",
    "            # initialize relative positional embeddings\n",
    "            self.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, dmodel_per_head), requires_grad = True)\n",
    "            self.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, dmodel_per_head), requires_grad = True)\n",
    "            \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, H, W, _ = x.shape # X的shape = (B, H, W, dmodel) = (1, 16, 16, 768)\n",
    "        qkv_combine = self.qkv(x) # shape = (B, H, W, 3*dmodel)，即用一个线性层生成qkv_combine\n",
    "        qkv_combine = qkv_combine.reshape(B, H*W, 3, self.num_heads, -1) # shape = (B, H*W, 3, num_heads, dmodel_per_head)\n",
    "        qkv_combine = qkv_combine.permute(2, 0, 3, 1, 4) # shape = (3, B, num_heads, H*W, dmodel_per_head)\n",
    "        qkv_combine = qkv_combine.reshape(3, B*self.num_heads, H*W, -1) # shape = (3, B*num_heads, H*W, dmodel_per_head)\n",
    "        q, k, v = qkv_combine.unbind(0) # q, k, v shape = (B*num_heads, H*W, dmodel_per_head)\n",
    "        \n",
    "        k_trans = k.transpose(-2, -1) # k_trans shape = (B*num_heads, dmodel_per_head, H*W)\n",
    "        scores = torch.matmul(q, k_trans) / self.scale # scores shape = (B*num_heads, H*W, H*W)\n",
    "        \n",
    "        # 如果使用了相对位置编码则启用\n",
    "        if self.use_rel_pos:\n",
    "            scores = add_decomposed_rel_pos(attn=scores, q=q, rel_pos_h=self.rel_pos_h, rel_pos_w=self.rel_pos_w, q_size=(H, W), k_size=(H, W))\n",
    "        \n",
    "        weights = torch.softmax(scores, dim = -1) # 在(B*num_heads, H*W, H*W)的最后一个维度上计算softmax。至于为什么已经推过了\n",
    "        output = torch.matmul(weights, v) # shape = (B*num_heads, H*W, dmodel_per_head)\n",
    "        output = output.view(B, self.num_heads, H, W, -1) # shape = (B, num_heads, H, W, dmodel_per_head)\n",
    "        output = output.permute(0, 2, 3, 1, 4) # (B, H, W, num_heads, dmodel_per_head)\n",
    "        output = output.reshape(B, H, W, -1) # shape = (B, H, W, dmodel)\n",
    "        output = self.output_linear(output)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d8a85cb9-4859-4b2e-8f84-e33b44be44c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 16, 768])\n",
      "torch.Size([1, 16, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "# 4.1 测试Attention类\n",
    "\n",
    "input = torch.randn((1, 16, 16, 768))\n",
    "print(input.shape)\n",
    "\n",
    "dmodel = input.shape[-1]\n",
    "num_heads = 8\n",
    "qkv_bias = True\n",
    "use_rel_pos = False\n",
    "rel_pos_zero_init = False\n",
    "\n",
    "attn = Attention(dmodel=dmodel, num_heads=num_heads, qkv_bias=qkv_bias, use_rel_pos=use_rel_pos, rel_pos_zero_init=rel_pos_zero_init)\n",
    "\n",
    "output = attn(input)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
